{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "current_directory = os.getcwd()        \n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "sys.path.append(parent_directory)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import dense_to_sparse, add_self_loops\n",
    "\n",
    "import math\n",
    "\n",
    "from preprocess.BaselinePrerocess import baseline_process\n",
    "from preprocess.GraphTransformerPrerocess import graph_water_transformer_cov_process_for_gate_predictor\n",
    "from preprocess.graph import graph_topology_5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hours = 72\n",
    "K = 24 \n",
    "masked_value = 1e-10\n",
    "split_1 = 0.8\n",
    "split_2 = 0.9\n",
    "sigma2 = 0.1\n",
    "epsilon = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_mask, val_X_mask, test_X_mask, \\\n",
    "train_ws_y, val_ws_y, test_ws_y, \\\n",
    "scaler, ws_scaler = baseline_process(n_hours, K, masked_value, split_1, split_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['WS_S1', 'WS_S4', 'FLOW_S25A', 'GATE_S25A', 'HWS_S25A', 'TWS_S25A',\n",
      "       'FLOW_S25B', 'GATE_S25B', 'GATE_S25B2', 'HWS_S25B', 'TWS_S25B',\n",
      "       'PUMP_S25B', 'FLOW_S26', 'GATE_S26_1', 'GATE_S26_2', 'HWS_S26',\n",
      "       'TWS_S26', 'PUMP_S26', 'MEAN_RAIN'],\n",
      "      dtype='object')\n",
      "train_tws/val_tws/test_tws: (77069, 5, 72) (9634, 5, 72) (19268, 5, 72) \n",
      " train_cov/val_cov/test_cov: (77069, 96, 12) (9634, 96, 12) (19268, 96, 12) \n",
      " train_ws_y/val_ws_y/test_ws_y: (77069, 96) (9634, 96) (19268, 96) \n",
      "  train_gate_pump_y/val_gate_pump_y/test_gate_pump_y: (77069, 24, 7) (9634, 24, 7) (19268, 24, 7)\n"
     ]
    }
   ],
   "source": [
    "train_cov, val_cov, test_cov, \\\n",
    "train_tws_reshape, val_tws_reshape, test_tws_reshape, \\\n",
    "train_gate_pump_y, val_gate_pump_y, test_gate_pump_y, \\\n",
    "train_ws_y, val_ws_y, test_ws_y, \\\n",
    "scaler, ws_scaler, gate_scalar = graph_water_transformer_cov_process_for_gate_predictor(n_hours, K, masked_value, split_1, split_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_indices: [0 0 0 0 1 1 2 2 3 3 4 4] \n",
      "neighbor_indices: [1 2 3 4 0 2 0 1 0 4 0 3]\n",
      "number of nodes: 5, number of edges: 12\n"
     ]
    }
   ],
   "source": [
    "train_adj_mat, val_adj_mat, test_adj_mat = graph_topology_5(n_hours, K, sigma2, epsilon, len(train_ws_y), len(val_ws_y), len(test_ws_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 12)\n"
     ]
    }
   ],
   "source": [
    "# ===== model parameters ======\n",
    "head_size = 96*2\n",
    "num_heads = 3\n",
    "ff_dim = 96\n",
    "num_transformer_blocks = 1\n",
    "dropout = 0.5\n",
    "atte_reg = 1e-2\n",
    "l1_reg = 1e-5\n",
    "l2_reg = 1e-5\n",
    "gcn_unit1 = 32\n",
    "gcn_unit2 = 16\n",
    "lstm_units = 32\n",
    "learning_rate = 5e-4\n",
    "decay_steps = 10000\n",
    "decay_rate = 0.95\n",
    "PATIENCE = 500\n",
    "EPOCHS = 3000\n",
    "BATCH = 512\n",
    "\n",
    "input_shape = train_cov.shape[1:]\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = model(input)\n",
    "# loss = criterion(output, target)\n",
    "# loss = loss + torch.norm(model.layer.weight, p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 96, 12])\n",
      "torch.Size([1, 12, 96])\n",
      "torch.Size([1, 96, 12])\n"
     ]
    }
   ],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_shape, num_heads, dropout, epsilon):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        self.MultAtten = nn.MultiheadAttention(\n",
    "            embed_dim=input_shape[0],\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.Dropout1 = nn.Dropout(dropout)\n",
    "        self.layer_norm1 = nn.LayerNorm(input_shape[::-1], epsilon)\n",
    "\n",
    "        self.conv1d_1 = nn.Conv1d(input_shape[0], ff_dim, 3, 1, 1)\n",
    "        self.Dropout2 = nn.Dropout(dropout)\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.conv1d_2 = nn.Conv1d(ff_dim, input_shape[-1], 3, 1, 1)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.layer_norm2 = nn.LayerNorm(input_shape[::-1], epsilon)\n",
    "    def forward(self, inputs):\n",
    "        print(inputs.shape)\n",
    "        inputs = inputs.permute(0, 2, 1)\n",
    "        x, _ = self.MultAtten(inputs, inputs, inputs)\n",
    "        x = self.Dropout1(x)\n",
    "        res = x + inputs\n",
    "        x = self.layer_norm1(res)\n",
    "\n",
    "        print(x.shape)\n",
    "        # Feed Forward Part\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x  = self.conv1d_1(x)\n",
    "        x = self.Dropout2(x)\n",
    "        x = self.act1(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        x = self.conv1d_2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.layer_norm2(res)\n",
    "        res = x + res\n",
    "        res = res.permute(0, 2, 1)\n",
    "        return res\n",
    "\n",
    "inputs = torch.rand([1, 96, 12])\n",
    "model = TransformerEncoder(input_shape, num_heads, dropout, epsilon)\n",
    "cov = model(inputs)\n",
    "print(cov.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73068"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 96, 12])\n",
      "torch.Size([1, 12, 96])\n",
      "+---------------------------+------------+\n",
      "|            Name           | Parameters |\n",
      "+---------------------------+------------+\n",
      "|  MultAtten.in_proj_weight |   27648    |\n",
      "|   MultAtten.in_proj_bias  |    288     |\n",
      "| MultAtten.out_proj.weight |    9216    |\n",
      "|  MultAtten.out_proj.bias  |     96     |\n",
      "|     layer_norm1.weight    |    1152    |\n",
      "|      layer_norm1.bias     |    1152    |\n",
      "|      conv1d_1.weight      |   27648    |\n",
      "|       conv1d_1.bias       |     96     |\n",
      "|      conv1d_2.weight      |    3456    |\n",
      "|       conv1d_2.bias       |     12     |\n",
      "|     layer_norm2.weight    |    1152    |\n",
      "|      layer_norm2.bias     |    1152    |\n",
      "+---------------------------+------------+\n",
      "Total Trainable Params: 73068\n",
      "73068\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.parameter import UninitializedParameter\n",
    "import torch\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Name\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params += params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "\n",
    "model.forward(inputs)\n",
    "\n",
    "total_params = count_parameters(model)\n",
    "print(total_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 96])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov = model(inputs)\n",
    "\n",
    "fc1 = nn.Linear(input_shape[1], 5)\n",
    "cov = fc1(cov)\n",
    "\n",
    "conv_reshape = cov.view(-1, 5, input_shape[0])\n",
    "conv_reshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 32])\n",
      "torch.Size([2, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "inp_lap = torch.rand([5, 5])\n",
    "inp_seq = torch.rand([2, 5, 72])\n",
    "\n",
    "edge_index = dense_to_sparse(inp_lap)[0]\n",
    "# print(edge_index)\n",
    "edge_index, _ = add_self_loops(edge_index, num_nodes=inp_lap.shape[0])\n",
    "# print(edge_index)\n",
    "\n",
    "gcn1 = GCNConv(72, 32)\n",
    "gcn2 = GCNConv(32, 16)\n",
    "act1 = nn.ReLU()\n",
    "act2 = nn.ReLU()\n",
    "x = act1(gcn1(inp_seq, edge_index)) #(5, 32)\n",
    "print(x.shape)\n",
    "x = act2(gcn2(x, edge_index))  #(5, 16)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2336"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(gcn1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(72, lstm_units)\n",
    "xx = lstm(inp_seq)[0] #(5, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.concat([conv_reshape, x, xx], dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 144])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 144])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LuongAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LuongAttention, self).__init__()\n",
    "\n",
    "    def forward(self, query, value, key):\n",
    "        dim = query.size(-1)\n",
    "        scores = torch.bmm(query, key.transpose(-2, -1)) / math.sqrt(dim)\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        output = torch.bmm(attention_weights, value)\n",
    "        return output, attention_weights\n",
    "\n",
    "attention = LuongAttention()\n",
    "attention(x, x, x)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = nn.Flatten()\n",
    "x = flatten(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 96])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc = nn.LazyLinear(96)\n",
    "fc(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LuongAttention, self).__init__()\n",
    "\n",
    "    def forward(self, query, value, key):\n",
    "        dim = query.size(-1)\n",
    "        scores = torch.bmm(query, key.transpose(-2, -1)) / math.sqrt(dim)\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        output = torch.bmm(attention_weights, value)\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([96])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Graph_Water_Transformer_Cov(nn.Module):\n",
    "    def __init__(self, num_transformer_blocks, gcn_unit1, gcn_unit2):\n",
    "        super(Graph_Water_Transformer_Cov, self).__init__()\n",
    "\n",
    "        self.num_transformer_blocks = num_transformer_blocks\n",
    "        self.gcn_unit1 = gcn_unit1\n",
    "        self.gcn_unit2 = gcn_unit2\n",
    "        \n",
    "\n",
    "        self.transformer_encoder = TransformerEncoder(input_shape, num_heads, dropout, epsilon)\n",
    "        self.fc1 = nn.Linear(input_shape[1], 5)\n",
    "\n",
    "        self.gcn1 = GCNConv(72, self.gcn_unit1)\n",
    "        self.gcn2 = GCNConv(self.gcn_unit1, self.gcn_unit2)\n",
    "        self.gcn_act1 = nn.ReLU()\n",
    "        self.gcn_act2 = nn.ReLU()\n",
    "\n",
    "\n",
    "\n",
    "        self.lstm = nn.LSTM(72, lstm_units)\n",
    "        self.attention = LuongAttention()\n",
    "        self.final_fc = nn.LazyLinear(96)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, cov_inputs, inp_seq, inp_lap):\n",
    "        # ======================== covariates with transformer ========================\n",
    "        cov = cov_inputs\n",
    "        for _ in range(self.num_transformer_blocks):\n",
    "            cov = self.transformer_encoder(cov)\n",
    "        cov = self.fc1(cov)\n",
    "        conv_reshape = cov.view(-1, 5, input_shape[0])\n",
    "\n",
    "\n",
    "        # ======================== water levels with GNN ========================\n",
    "        # GCN\n",
    "        edge_index = dense_to_sparse(inp_lap)[0]\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=inp_lap.shape[0])\n",
    "        x = self.gcn_act1(self.gcn1(inp_seq, edge_index)) #(5, 32)\n",
    "        x = self.gcn_act2(self.gcn2(x, edge_index))  #(5, 16)\n",
    "\n",
    "        # LSTM\n",
    "        xx = self.lstm(inp_seq)[0] #(5, 32)\n",
    "\n",
    "        # ======================== CONCAT and Attention ========================\n",
    "        x = torch.concat([conv_reshape, x, xx], dim=2)\n",
    "        x = self.attention(x, x, x)[0]\n",
    "\n",
    "        x = self.final_fc(torch.flatten(x))\n",
    "        return x\n",
    "\n",
    "inputs = torch.rand([1, 96, 12])\n",
    "inp_lap = torch.rand([5, 5])\n",
    "inp_seq = torch.rand([1, 5, 72])\n",
    "\n",
    "model = Graph_Water_Transformer_Cov(1, gcn_unit1, gcn_unit2)\n",
    "model(inputs, inp_seq, inp_lap).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
